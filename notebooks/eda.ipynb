{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5762c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "952bc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADER MODULE\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "class DataLoader:\n",
    "    \"\"\"Handles efficient data loading with chunking support.\"\"\"\n",
    "   \n",
    "    def __init__(self, data_path, chunksize=50000):\n",
    "        self.data_path = data_path\n",
    "        self.chunksize = chunksize\n",
    "   \n",
    "    def load_in_chunks(self, columns=None, filters=None, dtype=None, parse_dates=None):\n",
    "        \"\"\"Load data in chunks to avoid memory errors.\"\"\"\n",
    "        print(f\"üìÇ Loading data in chunks of {self.chunksize:,}...\")\n",
    "       \n",
    "        chunks = []\n",
    "        total_rows = 0\n",
    "        chunks_processed = 0\n",
    "       \n",
    "        try:\n",
    "            for chunk in pd.read_csv(\n",
    "                self.data_path,\n",
    "                chunksize=self.chunksize,\n",
    "                usecols=columns,\n",
    "                dtype=dtype,\n",
    "                parse_dates=parse_dates\n",
    "            ):\n",
    "                # Apply filters during loading\n",
    "                if filters:\n",
    "                    for col, values in filters.items():\n",
    "                        if col in chunk.columns:\n",
    "                            chunk = chunk[chunk[col].isin(values)]\n",
    "               \n",
    "                if len(chunk) > 0:\n",
    "                    chunks.append(chunk)\n",
    "                    total_rows += len(chunk)\n",
    "                    chunks_processed += 1\n",
    "                   \n",
    "                    if chunks_processed % 10 == 0:\n",
    "                        print(f\" Processed {chunks_processed} chunks: {total_rows:,} rows retained\")\n",
    "           \n",
    "            df = pd.concat(chunks, ignore_index=True)\n",
    "            print(f\"‚úì Successfully loaded {len(df):,} rows\")\n",
    "            return df\n",
    "           \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error loading data: {str(e)}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e6fc5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEXT PROCESSOR MODULE\n",
    "# ============================================================================\n",
    "import re\n",
    "class TextProcessor:\n",
    "    \"\"\"Handles text cleaning and preprocessing.\"\"\"\n",
    "   \n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "       \n",
    "        text = str(text).lower()\n",
    "       \n",
    "        # Remove boilerplate patterns\n",
    "        boilerplate = [\n",
    "            r'i am writing to file a complaint',\n",
    "            r'i am filing this complaint',\n",
    "            r'i would like to file a complaint',\n",
    "            r'this is a complaint about',\n",
    "            r'xxxx+',\n",
    "            r'\\*\\*+'\n",
    "        ]\n",
    "        for pattern in boilerplate:\n",
    "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "       \n",
    "        # Clean special characters\n",
    "        text = re.sub(r'[^a-z0-9\\s.,!?\\'-]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "       \n",
    "        return text\n",
    "   \n",
    "    @staticmethod\n",
    "    def add_word_count(df, text_col):\n",
    "        \"\"\"Add word count column.\"\"\"\n",
    "        print(f\"üìù Calculating word counts...\")\n",
    "        df['word_count'] = df[text_col].fillna('').apply(\n",
    "            lambda x: len(str(x).split())\n",
    "        )\n",
    "        return df\n",
    "   \n",
    "    @staticmethod\n",
    "    def get_cleaning_stats(df, original_col, cleaned_col):\n",
    "        \"\"\"Get before/after cleaning statistics.\"\"\"\n",
    "        orig_len = df[original_col].str.len().mean()\n",
    "        clean_len = df[cleaned_col].str.len().mean()\n",
    "        reduction = (1 - clean_len / orig_len) * 100\n",
    "       \n",
    "        return {\n",
    "            'original_avg': orig_len,\n",
    "            'cleaned_avg': clean_len,\n",
    "            'reduction_pct': reduction\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00db44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYZER MODULE\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "class DataAnalyzer:\n",
    "    \"\"\"Handles data analysis and visualization.\"\"\"\n",
    "   \n",
    "    def __init__(self, output_dir='output'):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "   \n",
    "    def initial_exploration(self, df):\n",
    "        \"\"\"Perform initial data exploration.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"INITIAL DATA EXPLORATION\")\n",
    "        print(\"=\" * 70)\n",
    "       \n",
    "        print(f\"\\nüìä Dataset Overview:\")\n",
    "        print(f\" Total Records: {len(df):,}\")\n",
    "        print(f\" Total Columns: {len(df.columns)}\")\n",
    "        print(f\" Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "       \n",
    "        if 'Date received' in df.columns:\n",
    "            print(f\" Date Range: {df['Date received'].min()} to {df['Date received'].max()}\")\n",
    "       \n",
    "        print(\"\\nüìã Columns:\")\n",
    "        for idx, col in enumerate(df.columns, 1):\n",
    "            print(f\" {idx:2d}. {col}\")\n",
    "       \n",
    "        print(\"\\nüîç Missing Data:\")\n",
    "        missing = df.isnull().sum()\n",
    "        missing = missing[missing > 0].sort_values(ascending=False)\n",
    "        if len(missing) > 0:\n",
    "            for col, count in missing.items():\n",
    "                pct = count / len(df) * 100\n",
    "                print(f\" {col}: {count:,} ({pct:.1f}%)\")\n",
    "        else:\n",
    "            print(\" No missing data!\")\n",
    "       \n",
    "        print(\"\\nüìÑ Sample Data (first 2 rows):\")\n",
    "        print(df.head(2).to_string())\n",
    "   \n",
    "    def analyze_products(self, df):\n",
    "        \"\"\"Analyze product distribution with visualization.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"PRODUCT DISTRIBUTION ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "       \n",
    "        counts = df['Product'].value_counts()\n",
    "        print(f\"\\nüìä Total Unique Products: {len(counts)}\")\n",
    "        print(f\"\\nProduct Distribution:\")\n",
    "        for product, count in counts.items():\n",
    "            pct = count / len(df) * 100\n",
    "            print(f\" {product}: {count:,} ({pct:.1f}%)\")\n",
    "       \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "       \n",
    "        # Bar chart\n",
    "        ax = axes[0]\n",
    "        counts.plot(kind='barh', ax=ax, color='steelblue', edgecolor='black')\n",
    "        ax.set_xlabel('Number of Complaints', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Product', fontweight='bold', fontsize=12)\n",
    "        ax.set_title('Complaints by Product', fontweight='bold', fontsize=14, pad=15)\n",
    "        ax.invert_yaxis()\n",
    "       \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(counts.values):\n",
    "            ax.text(v + max(counts) * 0.01, i, f'{v:,}', va='center', fontsize=10)\n",
    "       \n",
    "        # Pie chart\n",
    "        ax = axes[1]\n",
    "        colors = plt.cm.Set3.colors\n",
    "        ax.pie(counts.values, labels=counts.index, autopct='%1.1f%%',\n",
    "               startangle=90, colors=colors, textprops={'fontsize': 10})\n",
    "        ax.set_title('Product Distribution (%)', fontweight='bold', fontsize=14, pad=15)\n",
    "       \n",
    "        plt.tight_layout()\n",
    "        filename = self.output_dir / 'product_distribution.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"\\n‚úì Saved: {filename}\")\n",
    "       \n",
    "        return counts\n",
    "   \n",
    "    def analyze_narratives(self, df, narrative_col):\n",
    "        \"\"\"Analyze narrative statistics with visualization.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"NARRATIVE ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "       \n",
    "        with_narrative = df[df[narrative_col].notna()]\n",
    "        without_narrative = df[df[narrative_col].isna()]\n",
    "       \n",
    "        print(f\"\\nüìä Narrative Coverage:\")\n",
    "        print(f\" WITH narratives: {len(with_narrative):,} ({len(with_narrative)/len(df)*100:.1f}%)\")\n",
    "        print(f\" WITHOUT narratives: {len(without_narrative):,} ({len(without_narrative)/len(df)*100:.1f}%)\")\n",
    "       \n",
    "        if 'word_count' in df.columns and len(with_narrative) > 0:\n",
    "            stats = with_narrative['word_count'].describe()\n",
    "            print(f\"\\nüìà Word Count Statistics:\")\n",
    "            print(f\" Mean: {stats['mean']:.0f} words\")\n",
    "            print(f\" Median: {stats['50%']:.0f} words\")\n",
    "            print(f\" Std: {stats['std']:.0f} words\")\n",
    "            print(f\" Min: {stats['min']:.0f} words\")\n",
    "            print(f\" Max: {stats['max']:.0f} words\")\n",
    "            print(f\" 25th: {stats['25%']:.0f} words\")\n",
    "            print(f\" 75th: {stats['75%']:.0f} words\")\n",
    "           \n",
    "            # Edge cases\n",
    "            very_short = (with_narrative['word_count'] < 10).sum()\n",
    "            very_long = (with_narrative['word_count'] > 1000).sum()\n",
    "            print(f\"\\nüîç Edge Cases:\")\n",
    "            print(f\" Very SHORT (<10 words): {very_short:,} ({very_short/len(with_narrative)*100:.1f}%)\")\n",
    "            print(f\" Very LONG (>1000 words): {very_long:,} ({very_long/len(with_narrative)*100:.1f}%)\")\n",
    "           \n",
    "            # Visualization\n",
    "            self._plot_narrative_analysis(with_narrative, without_narrative, df)\n",
    "   \n",
    "    def _plot_narrative_analysis(self, with_narrative, without_narrative, df):\n",
    "        \"\"\"Create narrative analysis visualizations.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "       \n",
    "        # 1. Histogram of word counts (‚â§500)\n",
    "        ax = axes[0, 0]\n",
    "        data = with_narrative['word_count']\n",
    "        filtered_data = data[data <= 500]\n",
    "        ax.hist(filtered_data, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        ax.axvline(data.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Mean: {data.mean():.0f}')\n",
    "        ax.axvline(data.median(), color='green', linestyle='--', linewidth=2,\n",
    "                   label=f'Median: {data.median():.0f}')\n",
    "        ax.set_xlabel('Word Count', fontweight='bold', fontsize=11)\n",
    "        ax.set_ylabel('Frequency', fontweight='bold', fontsize=11)\n",
    "        ax.set_title('Distribution of Narrative Lengths (‚â§500 words)',\n",
    "                    fontweight='bold', fontsize=12, pad=10)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "       \n",
    "        # 2. Box plot\n",
    "        ax = axes[0, 1]\n",
    "        ax.boxplot(data[data <= 500], vert=True)\n",
    "        ax.set_ylabel('Word Count', fontweight='bold', fontsize=11)\n",
    "        ax.set_title('Box Plot of Narrative Lengths (‚â§500 words)',\n",
    "                    fontweight='bold', fontsize=12, pad=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "       \n",
    "        # 3. Pie chart - with/without narratives\n",
    "        ax = axes[1, 0]\n",
    "        sizes = [len(with_narrative), len(without_narrative)]\n",
    "        labels = ['With Narrative', 'Without Narrative']\n",
    "        colors = ['#3498db', '#e74c3c']\n",
    "        ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90,\n",
    "               colors=colors, textprops={'fontsize': 11})\n",
    "        ax.set_title('Complaints: With vs Without Narratives',\n",
    "                    fontweight='bold', fontsize=12, pad=10)\n",
    "       \n",
    "        # 4. Length categories\n",
    "        ax = axes[1, 1]\n",
    "        bins = [0, 50, 100, 200, 500, float('inf')]\n",
    "        labels = ['Very Short\\n(0-50)', 'Short\\n(51-100)', 'Medium\\n(101-200)',\n",
    "                  'Long\\n(201-500)', 'Very Long\\n(>500)']\n",
    "        categories = pd.cut(data, bins=bins, labels=labels)\n",
    "        counts = categories.value_counts().sort_index()\n",
    "        ax.bar(range(len(counts)), counts.values, color='teal',\n",
    "               edgecolor='black', alpha=0.7)\n",
    "        ax.set_xticks(range(len(counts)))\n",
    "        ax.set_xticklabels(counts.index, fontsize=10)\n",
    "        ax.set_ylabel('Number of Complaints', fontweight='bold', fontsize=11)\n",
    "        ax.set_title('Narrative Length Categories', fontweight='bold',\n",
    "                    fontsize=12, pad=10)\n",
    "       \n",
    "        for i, v in enumerate(counts.values):\n",
    "            ax.text(i, v + max(counts.values) * 0.01, f'{v:,}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "       \n",
    "        plt.tight_layout()\n",
    "        filename = self.output_dir / 'narrative_analysis.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"‚úì Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa567e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN PROCESSOR (Updated for Memory Efficiency)\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ComplaintProcessor:\n",
    "    \"\"\"Main processor orchestrating the entire pipeline (memory-optimized).\"\"\"\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.loader = DataLoader(data_path, chunksize=50000)\n",
    "        self.text_proc = TextProcessor()\n",
    "        self.analyzer = DataAnalyzer()\n",
    "\n",
    "        # 5 specific products as per requirements\n",
    "        self.target_products = [\n",
    "            'Credit card',\n",
    "            'Credit card or prepaid card',\n",
    "            'Student loan',\n",
    "            'Checking or savings account',\n",
    "            'Money transfer, virtual currency, or money service'\n",
    "        ]\n",
    "\n",
    "        self.df_filtered = None  # Only this dataset is retained\n",
    "\n",
    "    def load_and_filter_data(self):\n",
    "        \"\"\"Load data in chunks AND filter for target products + valid narratives.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"LOADING & FILTERING DATA (TARGET PRODUCTS ONLY)\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        columns = [\n",
    "            'Date received', 'Product', 'Consumer complaint narrative', 'Complaint ID'\n",
    "        ]\n",
    "\n",
    "        dtypes = {\n",
    "            'Product': 'category',\n",
    "            'Consumer complaint narrative': 'object',\n",
    "            'Complaint ID': 'int64'\n",
    "        }\n",
    "\n",
    "        # Apply product filter DURING loading\n",
    "        filters = {'Product': self.target_products}\n",
    "\n",
    "        print(f\"üìå Target products:\")\n",
    "        for i, prod in enumerate(self.target_products, 1):\n",
    "            print(f\" {i}. {prod}\")\n",
    "\n",
    "        print(f\"\\nüìÇ Loading only target products in chunks...\")\n",
    "        df_initial = self.loader.load_in_chunks(\n",
    "            columns=columns,\n",
    "            filters=filters,\n",
    "            dtype=dtypes,\n",
    "            parse_dates=['Date received']\n",
    "        )\n",
    "\n",
    "        if df_initial is None or len(df_initial) == 0:\n",
    "            print(\"‚úó No data loaded for target products.\")\n",
    "            return False\n",
    "\n",
    "        print(f\"‚úì Loaded {len(df_initial):,} complaints for target products.\")\n",
    "\n",
    "        # Remove empty/NaN narratives\n",
    "        narrative_col = 'Consumer complaint narrative'\n",
    "        before = len(df_initial)\n",
    "        self.df_filtered = df_initial[\n",
    "            df_initial[narrative_col].notna() &\n",
    "            (df_initial[narrative_col].str.strip() != '')\n",
    "        ].copy()\n",
    "\n",
    "        print(f\"‚úì After removing empty narratives: {len(self.df_filtered):,} complaints\")\n",
    "        print(f\"‚úì Removed: {before - len(self.df_filtered):,} empty narratives\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def run_filtered_eda(self):\n",
    "        \"\"\"Run EDA on the filtered dataset (only target products).\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EXPLORATORY DATA ANALYSIS - FILTERED DATASET\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # Basic overview\n",
    "        self.analyzer.initial_exploration(self.df_filtered)\n",
    "\n",
    "        # Product distribution\n",
    "        self.analyzer.analyze_products(self.df_filtered)\n",
    "\n",
    "        # Word count analysis\n",
    "        self.df_filtered = self.text_proc.add_word_count(\n",
    "            self.df_filtered, 'Consumer complaint narrative'\n",
    "        )\n",
    "\n",
    "        # Narrative analysis\n",
    "        self.analyzer.analyze_narratives(\n",
    "            self.df_filtered, 'Consumer complaint narrative'\n",
    "        )\n",
    "\n",
    "    def preprocess_text(self):\n",
    "        \"\"\"Preprocess complaint narratives.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TEXT PREPROCESSING\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        narrative_col = 'Consumer complaint narrative'\n",
    "\n",
    "        print(\"\\nüßπ Cleaning complaint narratives...\")\n",
    "        self.df_filtered['cleaned_narrative'] = self.df_filtered[narrative_col].apply(\n",
    "            self.text_proc.clean_text\n",
    "        )\n",
    "\n",
    "        # Show examples\n",
    "        print(\"\\nüîç Before/After Examples:\")\n",
    "        print(\"-\" * 70)\n",
    "        samples = self.df_filtered.sample(min(3, len(self.df_filtered)), random_state=42)\n",
    "\n",
    "        for idx, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "            orig = str(row[narrative_col])[:200]\n",
    "            clean = str(row['cleaned_narrative'])[:200]\n",
    "            print(f\"\\nExample {idx}:\")\n",
    "            print(f\"ORIGINAL ({len(str(row[narrative_col]))} chars):\")\n",
    "            print(f\" {orig}...\")\n",
    "            print(f\"CLEANED ({len(str(row['cleaned_narrative']))} chars):\")\n",
    "            print(f\" {clean}...\")\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "        # Statistics\n",
    "        stats = self.text_proc.get_cleaning_stats(\n",
    "            self.df_filtered, narrative_col, 'cleaned_narrative'\n",
    "        )\n",
    "\n",
    "        print(f\"\\nüìä Cleaning Statistics:\")\n",
    "        print(f\" Average original length: {stats['original_avg']:.0f} characters\")\n",
    "        print(f\" Average cleaned length: {stats['cleaned_avg']:.0f} characters\")\n",
    "        print(f\" Average reduction: {stats['reduction_pct']:.2f}%\")\n",
    "\n",
    "    def save_data(self, output_path='data/filtered_complaints.csv'):\n",
    "        \"\"\"Save processed data.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"SAVING PROCESSED DATA\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save only essential columns to reduce file size\n",
    "        save_cols = [\n",
    "            'Complaint ID', 'Date received', 'Product',\n",
    "            'Consumer complaint narrative', 'cleaned_narrative', 'word_count'\n",
    "        ]\n",
    "        self.df_filtered[save_cols].to_csv(output_path, index=False)\n",
    "\n",
    "        size_mb = Path(output_path).stat().st_size / 1024**2\n",
    "        print(f\"\\n‚úì Data saved successfully!\")\n",
    "        print(f\" File: {output_path}\")\n",
    "        print(f\" Size: {size_mb:.2f} MB\")\n",
    "        print(f\" Records: {len(self.df_filtered):,}\")\n",
    "        print(f\" Columns: {len(save_cols)}\")\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate summary report.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"GENERATING SUMMARY REPORT\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        narrative_col = 'Consumer complaint narrative'\n",
    "\n",
    "        report = f\"\"\"\n",
    "{'='*70}\n",
    "EDA AND PREPROCESSING SUMMARY REPORT\n",
    "CrediTrust Financial - Complaint Analysis System\n",
    "{'='*70}\n",
    "1. DATASET OVERVIEW\n",
    "   ‚Ä¢ Complaints with target products: {len(self.df_filtered):,}\n",
    "   ‚Ä¢ Date range: {self.df_filtered['Date received'].min()} to {self.df_filtered['Date received'].max()}\n",
    "2. TARGET PRODUCTS (5 Products)\n",
    "   ‚Ä¢ Credit card\n",
    "   ‚Ä¢ Credit card or prepaid card\n",
    "   ‚Ä¢ Student loan\n",
    "   ‚Ä¢ Checking or savings account\n",
    "   ‚Ä¢ Money transfer, virtual currency, or money service\n",
    "3. PRODUCT DISTRIBUTION\n",
    "\"\"\"\n",
    "        for product, count in self.df_filtered['Product'].value_counts().items():\n",
    "            pct = count / len(self.df_filtered) * 100\n",
    "            report += f\" ‚Ä¢ {product}: {count:,} ({pct:.1f}%)\\n\"\n",
    "\n",
    "        report += f\"\"\"\n",
    "4. NARRATIVE ANALYSIS\n",
    "   ‚Ä¢ Complaints WITH narratives: {len(self.df_filtered):,} (100% ‚Äî filtered)\n",
    "   ‚Ä¢ Average length: {self.df_filtered['word_count'].mean():.0f} words\n",
    "   ‚Ä¢ Median length: {self.df_filtered['word_count'].median():.0f} words\n",
    "   ‚Ä¢ Very short (<10 words): {(self.df_filtered['word_count'] < 10).sum():,}\n",
    "   ‚Ä¢ Very long (>1000 words): {(self.df_filtered['word_count'] > 1000).sum():,}\n",
    "5. TEXT PREPROCESSING APPLIED\n",
    "   ‚Ä¢ Lowercasing: ‚úì\n",
    "   ‚Ä¢ Boilerplate removal: ‚úì\n",
    "   ‚Ä¢ Special character cleaning: ‚úì\n",
    "   ‚Ä¢ Whitespace normalization: ‚úì\n",
    "   ‚Ä¢ Average text reduction: {(1 - self.df_filtered['cleaned_narrative'].str.len().mean() / self.df_filtered[narrative_col].str.len().mean()) * 100:.2f}%\n",
    "6. OUTPUT FILES GENERATED\n",
    "   ‚Ä¢ data/filtered_complaints.csv\n",
    "   ‚Ä¢ output/product_distribution.png\n",
    "   ‚Ä¢ output/narrative_analysis.png\n",
    "   ‚Ä¢ output/eda_summary_report.txt\n",
    "7. NEXT STEPS (Task 2)\n",
    "   ‚Ä¢ Chunk cleaned narratives\n",
    "   ‚Ä¢ Generate embeddings\n",
    "   ‚Ä¢ Store in FAISS vector DB\n",
    "   ‚Ä¢ Estimated chunks: ~{len(self.df_filtered) * 3:,}\n",
    "{'='*70}\n",
    "END OF REPORT\n",
    "{'='*70}\n",
    "        \"\"\"\n",
    "\n",
    "        print(report)\n",
    "\n",
    "        report_path = Path('output/eda_summary_report.txt')\n",
    "        report_path.parent.mkdir(exist_ok=True)\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "\n",
    "        print(f\"‚úì Report saved: {report_path}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run complete pipeline (memory-safe).\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPLAINT ANALYSIS PIPELINE - TASK 1 (MEMORY OPTIMIZED)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        Path('output').mkdir(exist_ok=True)\n",
    "\n",
    "        # Step 1: Load and filter in one pass\n",
    "        if not self.load_and_filter_data():\n",
    "            print(\"\\n‚úó Pipeline failed during data loading/filtering\")\n",
    "            return False\n",
    "\n",
    "        # Step 2: EDA on filtered data\n",
    "        self.run_filtered_eda()\n",
    "\n",
    "        # Step 3: Preprocess text\n",
    "        self.preprocess_text()\n",
    "\n",
    "        # Step 4: Save data\n",
    "        self.save_data()\n",
    "\n",
    "        # Step 5: Generate report\n",
    "        self.generate_report()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"‚úì PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"\\nGenerated Files:\")\n",
    "        print(\" ‚Ä¢ data/filtered_complaints.csv\")\n",
    "        print(\" ‚Ä¢ output/product_distribution.png\")\n",
    "        print(\" ‚Ä¢ output/narrative_analysis.png\")\n",
    "        print(\" ‚Ä¢ output/eda_summary_report.txt\")\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c92ac549",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ComplaintProcessor('../data/raw/complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bb901a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPLAINT ANALYSIS PIPELINE - TASK 1 (MEMORY OPTIMIZED)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "LOADING & FILTERING DATA (TARGET PRODUCTS ONLY)\n",
      "======================================================================\n",
      "üìå Target products:\n",
      " 1. Credit card\n",
      " 2. Credit card or prepaid card\n",
      " 3. Student loan\n",
      " 4. Checking or savings account\n",
      " 5. Money transfer, virtual currency, or money service\n",
      "\n",
      "üìÇ Loading only target products in chunks...\n",
      "üìÇ Loading data in chunks of 50,000...\n",
      " Processed 10 chunks: 19,676 rows retained\n",
      " Processed 20 chunks: 47,602 rows retained\n",
      " Processed 30 chunks: 72,784 rows retained\n",
      " Processed 40 chunks: 115,870 rows retained\n",
      " Processed 50 chunks: 155,081 rows retained\n",
      " Processed 60 chunks: 182,656 rows retained\n",
      " Processed 70 chunks: 218,477 rows retained\n",
      " Processed 80 chunks: 263,404 rows retained\n",
      " Processed 90 chunks: 312,973 rows retained\n",
      " Processed 100 chunks: 369,679 rows retained\n",
      " Processed 110 chunks: 434,457 rows retained\n",
      " Processed 120 chunks: 508,918 rows retained\n",
      " Processed 130 chunks: 584,686 rows retained\n",
      " Processed 140 chunks: 656,516 rows retained\n",
      " Processed 150 chunks: 721,053 rows retained\n",
      " Processed 160 chunks: 792,591 rows retained\n",
      " Processed 170 chunks: 845,707 rows retained\n",
      " Processed 180 chunks: 884,539 rows retained\n",
      " Processed 190 chunks: 953,509 rows retained\n",
      "‚úì Successfully loaded 979,016 rows\n",
      "‚úì Loaded 979,016 complaints for target products.\n",
      "‚úì After removing empty narratives: 480,050 complaints\n",
      "‚úì Removed: 498,966 empty narratives\n",
      "\n",
      "======================================================================\n",
      "EXPLORATORY DATA ANALYSIS - FILTERED DATASET\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "INITIAL DATA EXPLORATION\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset Overview:\n",
      " Total Records: 480,050\n",
      " Total Columns: 4\n",
      " Memory Usage: 597.34 MB\n",
      " Date Range: 2015-03-19 00:00:00 to 2025-06-15 00:00:00\n",
      "\n",
      "üìã Columns:\n",
      "  1. Date received\n",
      "  2. Product\n",
      "  3. Consumer complaint narrative\n",
      "  4. Complaint ID\n",
      "\n",
      "üîç Missing Data:\n",
      " No missing data!\n",
      "\n",
      "üìÑ Sample Data (first 2 rows):\n",
      "    Date received                      Product                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Consumer complaint narrative  Complaint ID\n",
      "119    2025-06-13                  Credit card                                                                         A XXXX XXXX card was opened under my name by a fraudster. I received a notice from XXXX  that an account was just opened under my name. I reached out to XXXX XXXX to state that this activity was unauthorized and not me. XXXX XXXX confirmed this was fraudulent and immediately closed the card. However, they have failed to remove this from the three credit agencies and this fraud is now impacting my credit score based on a hard credit pull done by XXXX XXXX that was done by a fraudster.      14069121\n",
      "130    2025-06-13  Checking or savings account  I made the mistake of using my wellsfargo debit card to depsit funds Into XXXXXXXX ATM machine outside their branch. \\n\\nI went into the branch and was told they couldn't help and had to phone the customer service for help. I did this and was told I was helped gave all the info for the time terminal id aact # s, XXXX was able to find the transaction and give me this info, he said the dispute would take a few days. \\n\\nI waited a few days and got a letter stating my dispute was rejected. I went back into XXXX and they said they never got the transaction.      14061897\n",
      "\n",
      "======================================================================\n",
      "PRODUCT DISTRIBUTION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìä Total Unique Products: 5\n",
      "\n",
      "Product Distribution:\n",
      " Checking or savings account: 140,319 (29.2%)\n",
      " Credit card or prepaid card: 108,667 (22.6%)\n",
      " Money transfer, virtual currency, or money service: 97,188 (20.2%)\n",
      " Credit card: 80,667 (16.8%)\n",
      " Student loan: 53,209 (11.1%)\n",
      "\n",
      "‚úì Saved: output\\product_distribution.png\n",
      "üìù Calculating word counts...\n",
      "\n",
      "======================================================================\n",
      "NARRATIVE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìä Narrative Coverage:\n",
      " WITH narratives: 480,050 (100.0%)\n",
      " WITHOUT narratives: 0 (0.0%)\n",
      "\n",
      "üìà Word Count Statistics:\n",
      " Mean: 207 words\n",
      " Median: 139 words\n",
      " Std: 228 words\n",
      " Min: 1 words\n",
      " Max: 6469 words\n",
      " 25th: 84 words\n",
      " 75th: 258 words\n",
      "\n",
      "üîç Edge Cases:\n",
      " Very SHORT (<10 words): 2,072 (0.4%)\n",
      " Very LONG (>1000 words): 5,691 (1.2%)\n",
      "‚úì Saved: output\\narrative_analysis.png\n",
      "\n",
      "======================================================================\n",
      "TEXT PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "üßπ Cleaning complaint narratives...\n",
      "\n",
      "üîç Before/After Examples:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 1:\n",
      "ORIGINAL (576 chars):\n",
      " Hi, I noticed today I have a credit card account on my credit report belonging to Elan Financial Services stating it was opened in 2014 on XX/XX/XXXX. At this date I was 14 years old and can say with ...\n",
      "CLEANED (560 chars):\n",
      " hi, i noticed today i have a credit card account on my credit report belonging to elan financial services stating it was opened in 2014 on xx xx . at this date i was 14 years old and can say with conf...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "ORIGINAL (218 chars):\n",
      " I did not received legible documentation and chase has said they communicated to us and our attorney about the fraud, but they havent. Chase XXXX is lying through documents they have provided about co...\n",
      "CLEANED (213 chars):\n",
      " i did not received legible documentation and chase has said they communicated to us and our attorney about the fraud, but they havent. chase is lying through documents they have provided about communi...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "ORIGINAL (2390 chars):\n",
      " I currently work XXXX XXXX XXXX XXXX XXXX XXXX and they have available a prepaid debit card XXXX XXXX XXXXXXXX XXXX XXXX which is operated by Payfare. On XX/XX/XXXX my debit card was charged {$86.00} ...\n",
      "CLEANED (2231 chars):\n",
      " i currently work and they have available a prepaid debit card which is operated by payfare. on xx xx my debit card was charged 86.00 by a company called but this charge was not familiar to me and i ha...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Cleaning Statistics:\n",
      " Average original length: 1155 characters\n",
      " Average cleaned length: 1101 characters\n",
      " Average reduction: 4.68%\n",
      "\n",
      "======================================================================\n",
      "SAVING PROCESSED DATA\n",
      "======================================================================\n",
      "\n",
      "‚úì Data saved successfully!\n",
      " File: data/filtered_complaints.csv\n",
      " Size: 1059.30 MB\n",
      " Records: 480,050\n",
      " Columns: 6\n",
      "\n",
      "======================================================================\n",
      "GENERATING SUMMARY REPORT\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "EDA AND PREPROCESSING SUMMARY REPORT\n",
      "CrediTrust Financial - Complaint Analysis System\n",
      "======================================================================\n",
      "1. DATASET OVERVIEW\n",
      "   ‚Ä¢ Complaints with target products: 480,050\n",
      "   ‚Ä¢ Date range: 2015-03-19 00:00:00 to 2025-06-15 00:00:00\n",
      "2. TARGET PRODUCTS (5 Products)\n",
      "   ‚Ä¢ Credit card\n",
      "   ‚Ä¢ Credit card or prepaid card\n",
      "   ‚Ä¢ Student loan\n",
      "   ‚Ä¢ Checking or savings account\n",
      "   ‚Ä¢ Money transfer, virtual currency, or money service\n",
      "3. PRODUCT DISTRIBUTION\n",
      " ‚Ä¢ Checking or savings account: 140,319 (29.2%)\n",
      " ‚Ä¢ Credit card or prepaid card: 108,667 (22.6%)\n",
      " ‚Ä¢ Money transfer, virtual currency, or money service: 97,188 (20.2%)\n",
      " ‚Ä¢ Credit card: 80,667 (16.8%)\n",
      " ‚Ä¢ Student loan: 53,209 (11.1%)\n",
      "\n",
      "4. NARRATIVE ANALYSIS\n",
      "   ‚Ä¢ Complaints WITH narratives: 480,050 (100% ‚Äî filtered)\n",
      "   ‚Ä¢ Average length: 207 words\n",
      "   ‚Ä¢ Median length: 139 words\n",
      "   ‚Ä¢ Very short (<10 words): 2,072\n",
      "   ‚Ä¢ Very long (>1000 words): 5,691\n",
      "5. TEXT PREPROCESSING APPLIED\n",
      "   ‚Ä¢ Lowercasing: ‚úì\n",
      "   ‚Ä¢ Boilerplate removal: ‚úì\n",
      "   ‚Ä¢ Special character cleaning: ‚úì\n",
      "   ‚Ä¢ Whitespace normalization: ‚úì\n",
      "   ‚Ä¢ Average text reduction: 4.68%\n",
      "6. OUTPUT FILES GENERATED\n",
      "   ‚Ä¢ data/filtered_complaints.csv\n",
      "   ‚Ä¢ output/product_distribution.png\n",
      "   ‚Ä¢ output/narrative_analysis.png\n",
      "   ‚Ä¢ output/eda_summary_report.txt\n",
      "7. NEXT STEPS (Task 2)\n",
      "   ‚Ä¢ Chunk cleaned narratives\n",
      "   ‚Ä¢ Generate embeddings\n",
      "   ‚Ä¢ Store in FAISS vector DB\n",
      "   ‚Ä¢ Estimated chunks: ~1,440,150\n",
      "======================================================================\n",
      "END OF REPORT\n",
      "======================================================================\n",
      "        \n",
      "‚úì Report saved: output\\eda_summary_report.txt\n",
      "\n",
      "======================================================================\n",
      "‚úì PIPELINE COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Generated Files:\n",
      " ‚Ä¢ data/filtered_complaints.csv\n",
      " ‚Ä¢ output/product_distribution.png\n",
      " ‚Ä¢ output/narrative_analysis.png\n",
      " ‚Ä¢ output/eda_summary_report.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
