
======================================================================
EDA AND PREPROCESSING SUMMARY REPORT
CrediTrust Financial - Complaint Analysis System
======================================================================
1. DATASET OVERVIEW
   • Complaints with target products: 480,050
   • Date range: 2015-03-19 00:00:00 to 2025-06-15 00:00:00
2. TARGET PRODUCTS (5 Products)
   • Credit card
   • Credit card or prepaid card
   • Student loan
   • Checking or savings account
   • Money transfer, virtual currency, or money service
3. PRODUCT DISTRIBUTION
 • Checking or savings account: 140,319 (29.2%)
 • Credit card or prepaid card: 108,667 (22.6%)
 • Money transfer, virtual currency, or money service: 97,188 (20.2%)
 • Credit card: 80,667 (16.8%)
 • Student loan: 53,209 (11.1%)

4. NARRATIVE ANALYSIS
   • Complaints WITH narratives: 480,050 (100% — filtered)
   • Average length: 207 words
   • Median length: 139 words
   • Very short (<10 words): 2,072
   • Very long (>1000 words): 5,691
5. TEXT PREPROCESSING APPLIED
   • Lowercasing: ✓
   • Boilerplate removal: ✓
   • Special character cleaning: ✓
   • Whitespace normalization: ✓
   • Average text reduction: 4.68%
6. OUTPUT FILES GENERATED
   • data/filtered_complaints.csv
   • output/product_distribution.png
   • output/narrative_analysis.png
   • output/eda_summary_report.txt
7. NEXT STEPS (Task 2)
   • Chunk cleaned narratives
   • Generate embeddings
   • Store in FAISS vector DB
   • Estimated chunks: ~1,440,150
======================================================================
END OF REPORT
======================================================================
        
======================================================================
TASK 2: TEXT CHUNKING, EMBEDDING & VECTOR INDEXING
======================================================================
1. SAMPLING STRATEGY
   • Sample size: 12,000 complaints (within 10k–15k range)
   • Method: Stratified sampling by 'Product' to ensure proportional representation
   • Goal: Maintain original product distribution while reducing compute load
   • Verification: Sample distribution matches population within ±2%

2. TEXT CHUNKING STRATEGY
   • Library: LangChain's RecursiveCharacterTextSplitter
   • Chunk size: 300 characters
   • Chunk overlap: 50 characters
   • Justification:
     - Smaller chunks (≤300 chars) preserve semantic coherence for complaint narratives
     - Overlap ensures context isn’t lost at sentence boundaries
     - Tested sizes: 200, 300, 500 → 300 gave best balance of context & granularity
     - Separators prioritize paragraph > sentence > word breaks

3. EMBEDDING MODEL CHOICE
   • Model: sentence-transformers/all-MiniLM-L6-v2
   • Why chosen:
     - Lightweight (22M params) but high quality for semantic similarity
     - 384-dimensional embeddings → efficient storage & fast search
     - Trained on diverse sentence-pair data (STS, NLI)
     - Ideal for English complaint narratives (short-to-medium length)
     - Widely used in production RAG systems (good community support)

4. VECTOR STORE & METADATA
   • Engine: FAISS (Facebook AI Similarity Search)
   • Index type: Flat L2 (exact search; sufficient for ~50k chunks)
   • Metadata stored per chunk:
     - complaint_id: Original CFPB ID
     - product: Financial product category
     - chunk_index: Position within original narrative
     - original_word_count: For context
     - chunk_word_count: For filtering
   • Total chunks indexed: 62,468
   • Final index size: ~91.5 MB (estimated)

5. NEXT STEPS
   • Implement retrieval with metadata filtering (e.g., "only credit card complaints")
   • Evaluate retrieval quality using relevance judgments
   • Consider HNSW index for faster approximate search if scale increases
======================================================================
        